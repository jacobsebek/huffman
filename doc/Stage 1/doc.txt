This is the full documentation of the source of this project

GENERAL INFO:
This program is using an implementation of the Huffman coding algorithm with
the heap priority queue to perform looseless compression (and following decompression) 
of plain text files with 8 BIT CHARACTER ENCODING up to 4 GIBIBYTES (-1 byte) of size. 
It reads these files and produces new input binary files, learn how to use this tool 
in the following tutorial.

TUTORIAL :
The program needs 3 parameters : 
    1. "-c" : Compress the input file, output to the output file 
       "-ci" : "-c" with debug information
       "-d" : Decompress the input file, output to the output file
       "-di" : "-d" with debug information
    2. Input file
    3. Output file

For compression, the input file should be a plain text file with 8bit encoding 
(256 character alphabet), the binary output file has by convention the .huf extension

For decompression, the input file should be a .huf file, generated by THE SAME EXACT
VERSION OF THIS PROGRAM, the output is a plain text file with 8bit encoding

Contents of "debug information" (1. parameter "-ci" or "-di") :
    Compression debug information :
        1. The whole binary huffman tree, where the number of dots before the node
           signifies the depth of the node. The node is formated in the following
           manner : < (<character> [<value>], <occurence>) > for leaf nodes and
           [ (--<occurence>--) ] for internal ones.
        2. The codes for all individual characters. Formated in the following
           manner : < <character> [<value>] : <full corresponding huffman code> >
        3. At most the first 16 characters of the input file separated by commas
           where each character is formatted in the following manner :
           < <character> [<value>]: <full corresponding huffman code> >
    Decompression debug information :
        1. The whole binary huffman tree, where the number of dots before the node
           signifies the depth of the node. The node is formated in the following
           manner : < (<character> [<value>]) > for leaf nodes and
           [ (--0--) ] for internal ones

The .huf file format :
    By convention, this is the format that .huf files should follow :
        <The letters "HUF" in ASCII (3 bytes)>
        <The number of total characters in the file (4 bytes)>
        <The mapping bit sequence (leaves*2-1 bits (flushed to next byte))>
        <The character fill sequence (leaves bytes)>
        <The encoded bitstream>
    
    The number of total characters in the file :
        This number is needed so the decompressor knows where to stop reading the 
        bitstream (there may be empty bits on the end)

    The mapping bit sequence :
        The format of the mapping bit sequence is read in this manner :
            1. Read the next bit
            2. In case it is 0, return an internal node, whose children are 
               [step 1] (left) and [step 2] (right)
               In the case it is 1, return a leaf node
    
    The character fill sequence :
        This sequence of 8 bit character is used to fill the empty tree with
        characters from left to right, in other words, every character corresponds
        to every leaf node (1) in the mapping bit sequence (in the same order)

    This is the format which is created by the compressor and read by the decompressor.
    The decompressor will not be able to read any other format.

TESTING INFO :
    This program is supposed to work on every file up to 4 GIBIBYTES of size (-1 byte).
    I do not say guranteed because I would hardly test this on a 4 gigabyte file right?
    The huffman codes can reach up to 256 bits in length in extreme cases, but I had to
    make sure this would always work. Even though most of the time 16 bits are more than
    enough to represent every code.
    The 256 bit bitstrings are implemented as following 'uint64_t bits[4]' and special
    bit-manipulation functions are used to manipulate this string. 

    The 4 main compression & decompression tests were performed on these file in the res/ directory :
        1. 256chars.txt : This is a simple text file containing values from 0 to 255, its main
           objective was to test the proper behavior of the tree when being essentialy full
        2. bible.txt : King James translation of the Bible, this file is 4 megabytes large,
           which is.. truly large
        3. E.coli : The complete genome of the Escherichia coli bacterium - This is a
           a good example of huffman coding excelling in its work, genomes are made OF
           only 4 letters, therefore, the file could be compressed up to 25% of the size!
        4. random100K.txt : A random string of all readable ASCII characters, this was
           used to test general efficiency with files without grammar

    bible.txt, E.coli.txt and random100K.txt (originally random.txt) are text files acquired
    from the canterbury corpus's Artificial corpus and Large corpus
    (http://corpus.canterbury.ac.nz/descriptions/#calgary)

    Results :
        - 256chars.txt   was compressed to 78.8% of its base size in < 0.01 seconds.
        - bible.txt      was compressed to 54.8% of its base size in 0.49 seconds.
        - E.coli         was compressed to 25.0% of its base size in 0.28 seconds.
        - random100K.txt was compressed to 75.1% of its base size in 0.02 seconds.

    I used the Visual Studio Code file compare tool to make sure the decompressed files
    were identical to the input ones.